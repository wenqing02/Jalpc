---
layout: post
tags: [卷积神经网络,algorithm]
title: 卷积神经网络
---

# 卷积神经网络

卷积神经网络最基本的结构由卷积层、激活层和池化层构成，其中卷积层用来模拟对特定图案的响应，池化层模拟ReLU是2012年之后深度学习中被用的最多的一个激活函数，它的函数表达式是：f(x)=max(0,x)。但它也不是完美的，因为在输入小于0的区域，即使有个很大的梯度传播过来也会戛然而止，即会出现Dying ReLU.针对这个问题，一种解决方案是，将ReLU小于0的部分改成一个斜率小于0大于-1的线性函数，即y=ax,对于Leaky ReLU，a是一个非常小的值，比如0.01。从本质上来讲，ReLU解决了梯度消失的原则是靠梯度等于或接近1，从而避免了连续相乘的结果衰减。

Dropout:随机失活法，在神经网络中的表现是：在隐藏层，在每次的梯度迭代中，让隐含层的每个单元以一定的概率p不被激活。

局部响应归一化：它是在某一层得到多通道的响应图后，对响应图上某一位置和邻近通道的值做归一化。但是随着之后更深结构的提出，局部响应归一化被认为并没有什么作用。

## GoogLeNet

GoogLeNet有22层，它在imagenet的识别率达到了人类的识别水平，它跳出了AlexNet的结构，创新性的提出了Inception模块。Inception模块里面主要做了两件事：第一件事是通过3x3池化，以及1x1、3x3、5x5这3中不同尺度的卷积核，一共4中方式对输入的特征响应图做了特征抽取。第二件事是为了降低计算量，同时让信息通过更少的连接传递以达到更加稀疏的特性，采用1x1卷积核进行降维。

## ResNet

ResNet解决了训练网络的退化问题，所谓退化问题就是：随着网络的层数加深到一定程度后，越深的网络反而效果越差，并不是过拟合和梯度传播衰减的问题。为了解决退化，ResNet提出了残差模块。残差是指预测值和观测值之间的差异，并不是误差，误差是观测值和真实值之间的差异。残差模块的大体思路是：既然单位映射在梯度下降框架中不起作用，那么索性直接把输入传到输出端，强行作为单位映射的部分，让可学习的网络作为另外一部分。从它的思路我们可以知道，数据经过了两条路线，一条是和一般网络类似的经过卷积层再传到输出，另一条是实现单位映射的直接连接的路线。这个模块很好的应对了退化的问题。
